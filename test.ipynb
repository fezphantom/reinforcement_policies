{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "special_states = {\n",
    "    (0, 0): ('blue', 5, (4, 0)),\n",
    "    (0, 4): ('green', 2.5, [(4, 4), (4, 0)]),\n",
    "    (4, 0): ('yellow', 0, None),\n",
    "    (4, 4): ('red', 0, None)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function (Explicit):\n",
      "[[5.         2.70014423 1.82231019 1.7490911  2.5       ]\n",
      " [2.54799512 1.84657392 1.4013395  1.29319283 1.44479288]\n",
      " [1.33383146 1.12556922 0.93829989 0.84979895 0.84535272]\n",
      " [0.60873665 0.62053358 0.57402868 0.50125543 0.41943533]\n",
      " [0.         0.30443842 0.35687398 0.26728621 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the possible actions and their effects\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "# Define the special states and their effects\n",
    "special_states = {\n",
    "    (0, 0): ('blue', 5, (4, 0)),\n",
    "    (0, 4): ('green', 2.5, [(4, 4), (4, 0)]),\n",
    "    (4, 0): ('yellow', 0, None),\n",
    "    (4, 4): ('red', 0, None)\n",
    "}\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action_effects[action]\n",
    "    new_state = (max(0, min(grid_size - 1, x + dx)), max(0, min(grid_size - 1, y + dy)))\n",
    "    return new_state\n",
    "\n",
    "def build_system():\n",
    "    A = np.zeros((grid_size * grid_size, grid_size * grid_size))\n",
    "    b = np.zeros(grid_size * grid_size)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state = (i, j)\n",
    "            state_idx = i * grid_size + j\n",
    "            if state in special_states:\n",
    "                if special_states[state][0] == 'blue':\n",
    "                    reward = special_states[state][1]\n",
    "                    next_state = special_states[state][2]\n",
    "                    next_state_idx = next_state[0] * grid_size + next_state[1]\n",
    "                    A[state_idx, state_idx] = 1\n",
    "                    b[state_idx] = reward + gamma * (np.ones(4) / 4).sum() * A[next_state_idx, state_idx]\n",
    "                elif special_states[state][0] == 'green':\n",
    "                    reward = special_states[state][1]\n",
    "                    next_states = special_states[state][2]\n",
    "                    next_state_idxs = [ns[0] * grid_size + ns[1] for ns in next_states]\n",
    "                    A[state_idx, state_idx] = 1\n",
    "                    b[state_idx] = reward + gamma * 0.5 * (A[next_state_idxs[0], state_idx] + A[next_state_idxs[1], state_idx])\n",
    "                else:\n",
    "                    A[state_idx, state_idx] = 1\n",
    "            else:\n",
    "                for action in actions:\n",
    "                    next_state = get_next_state(state, action)\n",
    "                    next_state_idx = next_state[0] * grid_size + next_state[1]\n",
    "                    A[state_idx, state_idx] += -1 / len(actions)\n",
    "                    A[state_idx, next_state_idx] += gamma / len(actions)\n",
    "    return A, b\n",
    "\n",
    "A, b = build_system()\n",
    "values = np.linalg.solve(A, b)\n",
    "value_function_explicit = values.reshape((grid_size, grid_size))\n",
    "\n",
    "print(\"Value function (Explicit):\")\n",
    "print(value_function_explicit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function (Iterative Policy Evaluation):\n",
      "[[ 5.          1.37799231  0.14677657  0.42693862  2.5       ]\n",
      " [ 1.2258432   0.1194069  -0.49159788 -0.43397486  0.12263994]\n",
      " [-0.34170217 -0.76736815 -1.05999062 -1.04313909 -0.8301821 ]\n",
      " [-0.71341583 -1.1066341  -1.31890936 -1.22591278 -0.90271797]\n",
      " [ 0.         -1.01771451 -1.31866084 -1.05486708  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, gamma, theta=1e-6):\n",
    "    value_function = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                state = (i, j)\n",
    "                v = value_function[state]\n",
    "                new_v = 0\n",
    "                if state in special_states:\n",
    "                    if special_states[state][0] == 'blue':\n",
    "                        reward = special_states[state][1]\n",
    "                        next_state = special_states[state][2]\n",
    "                        new_v = reward + gamma * value_function[next_state]\n",
    "                    elif special_states[state][0] == 'green':\n",
    "                        reward = special_states[state][1]\n",
    "                        next_states = special_states[state][2]\n",
    "                        new_v = reward + gamma * 0.5 * (value_function[next_states[0]] + value_function[next_states[1]])\n",
    "                else:\n",
    "                    for action in actions:\n",
    "                        next_state = get_next_state(state, action)\n",
    "                        new_v += 1 / len(actions) * (-0.2 + gamma * value_function[next_state])\n",
    "                value_function[state] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "# Initial policy: Equal probability for all actions\n",
    "policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "\n",
    "value_function_iterative = policy_evaluation(policy, gamma)\n",
    "\n",
    "print(\"Value function (Iterative Policy Evaluation):\")\n",
    "print(value_function_iterative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function (Value Iteration):\n",
      "[[ 5.00000000e+00  1.13750000e+00  2.20156250e-01  5.43750000e-01\n",
      "   2.50000000e+00]\n",
      " [ 1.13750000e+00  2.20156250e-01  2.28710937e-03  7.91406250e-02\n",
      "   5.43750000e-01]\n",
      " [ 2.20156250e-01  2.28710937e-03 -4.94568115e-02 -3.12041016e-02\n",
      "   7.91406250e-02]\n",
      " [ 2.28710937e-03 -4.94568115e-02 -6.17459927e-02 -5.74109741e-02\n",
      "  -3.12041016e-02]\n",
      " [ 0.00000000e+00 -5.00000000e-02 -6.18750000e-02 -5.00000000e-02\n",
      "   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid size and discount factor\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Define the possible actions and their effects\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "# Define the special states and their effects\n",
    "special_states = {\n",
    "    (0, 0): ('blue', 5, (4, 0)),\n",
    "    (0, 4): ('green', 2.5, [(4, 4), (4, 0)]),\n",
    "    (4, 0): ('yellow', 0, None),\n",
    "    (4, 4): ('red', 0, None)\n",
    "}\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action_effects[action]\n",
    "    new_state = (max(0, min(grid_size - 1, x + dx)), max(0, min(grid_size - 1, y + dy)))\n",
    "    return new_state\n",
    "\n",
    "def value_iteration(gamma, theta=1e-6):\n",
    "    value_function = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_value_function = value_function.copy()\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                state = (i, j)\n",
    "                v = value_function[state]\n",
    "                \n",
    "                if state in special_states:\n",
    "                    if special_states[state][0] == 'blue':\n",
    "                        reward = special_states[state][1]\n",
    "                        next_state = special_states[state][2]\n",
    "                        new_v = reward + gamma * value_function[next_state]\n",
    "                    elif special_states[state][0] == 'green':\n",
    "                        reward = special_states[state][1]\n",
    "                        next_states = special_states[state][2]\n",
    "                        new_v = reward + gamma * 0.5 * (value_function[next_states[0]] + value_function[next_states[1]])\n",
    "                    else:\n",
    "                        new_v = 0\n",
    "                else:\n",
    "                    new_v = max(\n",
    "                        (1 / len(actions)) * (-0.2 + gamma * value_function[get_next_state(state, action)]) for action in actions\n",
    "                    )\n",
    "                \n",
    "                new_value_function[state] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "        \n",
    "        value_function = new_value_function\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "value_function_value_iteration = value_iteration(gamma)\n",
    "\n",
    "print(\"Value function (Value Iteration):\")\n",
    "print(value_function_value_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value (explicit): (0, 0) with value 5.0\n",
      "Highest value (policy evaluation): (0, 0) with value 5.0\n",
      "Highest value (value iteration): (0, 0) with value 5.0\n"
     ]
    }
   ],
   "source": [
    "# Identify the states with the highest values\n",
    "max_explicit = np.unravel_index(np.argmax(value_function_explicit), value_function_explicit.shape)\n",
    "max_policy = np.unravel_index(np.argmax(value_function_iterative), value_function_iterative.shape)\n",
    "max_value_iter = np.unravel_index(np.argmax(value_function_value_iteration), value_function_value_iteration.shape)\n",
    "\n",
    "print(f\"Highest value (explicit): {max_explicit} with value {value_function_explicit[max_explicit]}\")\n",
    "print(f\"Highest value (policy evaluation): {max_policy} with value {value_function_iterative[max_policy]}\")\n",
    "print(f\"Highest value (value iteration): {max_value_iter} with value {value_function_value_iteration[max_value_iter]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "special_states = {\n",
    "    (0, 0): ('blue', 5, (4, 0)),\n",
    "    (0, 4): ('green', 2.5, [(4, 4), (4, 0)]),\n",
    "    (4, 0): ('yellow', 0, None),\n",
    "    (4, 4): ('red', 0, None)\n",
    "}\n",
    "\n",
    "def initialize_gridworld(grid_size, special_states):\n",
    "    rewards = np.zeros((grid_size, grid_size))\n",
    "    transitions = np.empty((grid_size, grid_size), dtype=object)\n",
    "    \n",
    "    for state, (color, reward, next_states) in special_states.items():\n",
    "        rewards[state] = reward\n",
    "        transitions[state] = next_states\n",
    "    \n",
    "    return rewards, transitions\n",
    "\n",
    "def get_next_state(state, action, grid_size, transitions):\n",
    "    x, y = state\n",
    "    dx, dy = action_effects[action]\n",
    "    new_state = (x + dx, y + dy)\n",
    "    \n",
    "    if new_state in transitions:\n",
    "        if isinstance(transitions[new_state], list):\n",
    "            new_state = transitions[new_state][np.random.choice(len(transitions[new_state]))]\n",
    "        else:\n",
    "            new_state = transitions[new_state]\n",
    "    \n",
    "    new_x, new_y = new_state\n",
    "    if new_x < 0 or new_x >= grid_size or new_y < 0 or new_y >= grid_size:\n",
    "        return state\n",
    "    return new_state\n",
    "\n",
    "def get_reward(state, rewards):\n",
    "    return rewards[state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Bellman Optimality):\n",
      " [[99.99998127  0.          0.          0.         49.99999064]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "Optimal Policy (Bellman Optimality):\n",
      " [[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Function (Policy Iteration):\n",
      " [[99.99998127  0.          0.          0.         49.99999064]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "Optimal Policy (Policy Iteration):\n",
      " [[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Function (Value Iteration with Policy Improvement):\n",
      " [[99.99998127  0.          0.          0.         49.99999064]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n",
      "Optimal Policy (Value Iteration with Policy Improvement):\n",
      " [[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "special_states = {\n",
    "    (0, 0): ('blue', 5, (4, 0)),\n",
    "    (0, 4): ('green', 2.5, [(4, 4), (4, 0)]),\n",
    "    (4, 0): ('yellow', 0, None),\n",
    "    (4, 4): ('red', 0, None)\n",
    "}\n",
    "\n",
    "def initialize_gridworld(grid_size, special_states):\n",
    "    rewards = np.zeros((grid_size, grid_size))\n",
    "    transitions = {}\n",
    "    \n",
    "    for state, (color, reward, next_states) in special_states.items():\n",
    "        rewards[state] = reward\n",
    "        transitions[state] = next_states\n",
    "    \n",
    "    return rewards, transitions\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state in special_states and special_states[state][2] is None\n",
    "\n",
    "def get_next_state(state, action, grid_size, transitions):\n",
    "    x, y = state\n",
    "    dx, dy = action_effects[action]\n",
    "    new_state = (x + dx, y + dy)\n",
    "    \n",
    "    if new_state in transitions:\n",
    "        if isinstance(transitions[new_state], list):\n",
    "            new_state = transitions[new_state][np.random.choice(len(transitions[new_state]))]\n",
    "        else:\n",
    "            new_state = transitions[new_state]\n",
    "    \n",
    "    if new_state is None:\n",
    "        return state\n",
    "\n",
    "    new_x, new_y = new_state\n",
    "    if new_x < 0 or new_x >= grid_size or new_y < 0 or new_y >= grid_size:\n",
    "        return state\n",
    "    return new_state\n",
    "\n",
    "def get_reward(state, rewards):\n",
    "    return rewards[state]\n",
    "\n",
    "### 1. Solving the Bellman Optimality Equation\n",
    "\n",
    "def bellman_optimality(grid_size, rewards, transitions, gamma=0.95, theta=1e-6):\n",
    "    value_function = np.zeros((grid_size, grid_size))\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_values = np.copy(value_function)\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state = (x, y)\n",
    "                if is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                action_values = []\n",
    "                for action in actions:\n",
    "                    next_state = get_next_state(state, action, grid_size, transitions)\n",
    "                    reward = get_reward(next_state, rewards)\n",
    "                    action_values.append(reward + gamma * value_function[next_state])\n",
    "                \n",
    "                new_values[state] = max(action_values)\n",
    "                policy[state] = np.argmax(action_values)\n",
    "                delta = max(delta, abs(value_function[state] - new_values[state]))\n",
    "        \n",
    "        value_function = new_values\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_function, policy\n",
    "\n",
    "# Initialize gridworld\n",
    "rewards, transitions = initialize_gridworld(grid_size, special_states)\n",
    "value_function_bellman, policy_bellman = bellman_optimality(grid_size, rewards, transitions)\n",
    "print(\"Value Function (Bellman Optimality):\\n\", value_function_bellman)\n",
    "print(\"Optimal Policy (Bellman Optimality):\\n\", policy_bellman)\n",
    "\n",
    "### 2. Policy Iteration with Iterative Policy Evaluation\n",
    "\n",
    "def policy_evaluation(policy, grid_size, rewards, transitions, gamma=0.95, theta=1e-6):\n",
    "    value_function = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_values = np.copy(value_function)\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state = (x, y)\n",
    "                if is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                action = actions[policy[state]]\n",
    "                next_state = get_next_state(state, action, grid_size, transitions)\n",
    "                reward = get_reward(next_state, rewards)\n",
    "                new_values[state] = reward + gamma * value_function[next_state]\n",
    "                delta = max(delta, abs(value_function[state] - new_values[state]))\n",
    "        \n",
    "        value_function = new_values\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "def policy_iteration(grid_size, rewards, transitions, gamma=0.95, theta=1e-6):\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    \n",
    "    while True:\n",
    "        value_function = policy_evaluation(policy, grid_size, rewards, transitions, gamma, theta)\n",
    "        \n",
    "        policy_stable = True\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state = (x, y)\n",
    "                if is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                old_action = policy[state]\n",
    "                action_values = []\n",
    "                for action in actions:\n",
    "                    next_state = get_next_state(state, action, grid_size, transitions)\n",
    "                    reward = get_reward(next_state, rewards)\n",
    "                    action_values.append(reward + gamma * value_function[next_state])\n",
    "                \n",
    "                policy[state] = np.argmax(action_values)\n",
    "                if old_action != policy[state]:\n",
    "                    policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "    \n",
    "    return value_function, policy\n",
    "\n",
    "# Initialize gridworld\n",
    "rewards, transitions = initialize_gridworld(grid_size, special_states)\n",
    "value_function_policy_iter, policy_policy_iter = policy_iteration(grid_size, rewards, transitions)\n",
    "print(\"Value Function (Policy Iteration):\\n\", value_function_policy_iter)\n",
    "print(\"Optimal Policy (Policy Iteration):\\n\", policy_policy_iter)\n",
    "\n",
    "### 3. Policy Improvement with Value Iteration\n",
    "\n",
    "def value_iteration_with_policy(grid_size, rewards, transitions, gamma=0.95, theta=1e-6):\n",
    "    value_function = np.zeros((grid_size, grid_size))\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_values = np.copy(value_function)\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                state = (x, y)\n",
    "                if is_terminal(state):\n",
    "                    continue\n",
    "                \n",
    "                action_values = []\n",
    "                for action in actions:\n",
    "                    next_state = get_next_state(state, action, grid_size, transitions)\n",
    "                    reward = get_reward(next_state, rewards)\n",
    "                    action_values.append(reward + gamma * value_function[next_state])\n",
    "                \n",
    "                new_values[state] = max(action_values)\n",
    "                policy[state] = np.argmax(action_values)\n",
    "                delta = max(delta, abs(value_function[state] - new_values[state]))\n",
    "        \n",
    "        value_function = new_values\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return value_function, policy\n",
    "\n",
    "# Initialize gridworld\n",
    "rewards, transitions = initialize_gridworld(grid_size, special_states)\n",
    "value_function_value_iter, policy_value_iter = value_iteration_with_policy(grid_size, rewards, transitions)\n",
    "print(\"Value Function (Value Iteration with Policy Improvement):\\n\", value_function_value_iter)\n",
    "print(\"Optimal Policy (Value Iteration with Policy Improvement):\\n\", policy_value_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define grid size\n",
    "size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Initialize rewards\n",
    "rewards = np.zeros((size, size))\n",
    "# Special rewards\n",
    "rewards[0, 1] = 5  # Blue square\n",
    "rewards[1, 2] = 2.5  # Green square\n",
    "\n",
    "# Initialize transitions\n",
    "transitions = np.zeros((size, size, 4, 2), dtype=int)\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        transitions[i, j, 0] = [max(i-1, 0), j]  # Up\n",
    "        transitions[i, j, 1] = [min(i+1, size-1), j]  # Down\n",
    "        transitions[i, j, 2] = [i, max(j-1, 0)]  # Left\n",
    "        transitions[i, j, 3] = [i, min(j+1, size-1)]  # Right\n",
    "\n",
    "# Special transitions\n",
    "transitions[0, 1] = [[4, 1], [4, 1], [4, 1], [4, 1]]  # Blue to Red\n",
    "transitions[1, 2] = [[2, 3], [2, 3], [2, 3], [2, 3]]  # Green to Yellow/Red with 0.5 prob each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Solving Bellman Equations):\n",
      "[[4.57697642 1.38417783 4.42820117 2.96990739 2.30973636]\n",
      " [3.4701911  4.11528987 1.96803432 2.79702832 2.13582561]\n",
      " [2.44887355 2.61039621 2.6211121  2.07161507 1.75035965]\n",
      " [1.78158568 1.80586643 1.75463693 1.5540897  1.41213503]\n",
      " [1.46508772 1.45702929 1.40687673 1.30514855 1.22924733]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import solve\n",
    "\n",
    "# Define the state space\n",
    "num_states = size * size\n",
    "states = [(i, j) for i in range(size) for j in range(size)]\n",
    "\n",
    "# Define the transition and reward matrices\n",
    "P = np.zeros((num_states, num_states))\n",
    "R = np.zeros(num_states)\n",
    "\n",
    "for idx, (i, j) in enumerate(states):\n",
    "    for action in range(4):\n",
    "        next_state = transitions[i, j, action]\n",
    "        next_idx = states.index(tuple(next_state))\n",
    "        P[idx, next_idx] += 0.25  # Equal probability for all actions\n",
    "        R[idx] += 0.25 * rewards[next_state[0], next_state[1]]\n",
    "\n",
    "# Solve the Bellman equation\n",
    "I = np.eye(num_states)\n",
    "V = solve(I - gamma * P, R)\n",
    "\n",
    "# Reshape V to 2D grid\n",
    "V_grid = V.reshape((size, size))\n",
    "print(\"Value Function (Solving Bellman Equations):\")\n",
    "print(V_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Iterative Policy Evaluation):\n",
      "[[1.78653436 0.39079888 2.48556839 1.11118596 0.57909762]\n",
      " [1.50973274 2.94279006 1.06883064 1.61401261 0.85916259]\n",
      " [0.97223796 1.52659738 1.71350234 1.1250855  0.73014412]\n",
      " [0.56464093 0.79924935 0.86265544 0.67954397 0.50002787]\n",
      " [0.3030647  0.41136758 0.4399386  0.37346964 0.29162969]]\n"
     ]
    }
   ],
   "source": [
    "def iterative_policy_evaluation(V, theta=1e-6):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                v = V[i, j]\n",
    "                V[i, j] = 0\n",
    "                for action in range(4):\n",
    "                    next_state = transitions[i, j, action]\n",
    "                    V[i, j] += 0.25 * (rewards[next_state[0], next_state[1]] + gamma * V[next_state[0], next_state[1]])\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V_iterative = np.zeros((size, size))\n",
    "V_iterative = iterative_policy_evaluation(V_iterative)\n",
    "print(\"Value Function (Iterative Policy Evaluation):\")\n",
    "print(V_iterative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Value Iteration):\n",
      "[[22.10246707 18.00259757 22.10246769 20.9973443  19.94747709]\n",
      " [20.99734371 22.10246769 18.00259757 19.94747709 18.95010323]\n",
      " [19.94747653 20.9973443  19.94747709 18.95010323 18.00259807]\n",
      " [18.9501027  19.94747709 18.95010323 18.00259807 17.10246817]\n",
      " [18.00259757 18.95010323 18.00259807 17.10246817 16.24734476]]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(V, theta=1e-6):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                v = V[i, j]\n",
    "                V[i, j] = max([\n",
    "                    rewards[transitions[i, j, action][0], transitions[i, j, action][1]] + gamma * V[transitions[i, j, action][0], transitions[i, j, action][1]]\n",
    "                    for action in range(4)\n",
    "                ])\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V_value_iteration = np.zeros((size, size))\n",
    "V_value_iteration = value_iteration(V_value_iteration)\n",
    "print(\"Value Function (Value Iteration):\")\n",
    "print(V_value_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Policy Iteration):\n",
      "[[3 0 2 2 2]\n",
      " [3 0 0 2 2]\n",
      " [3 0 0 2 2]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Value Function (Policy Iteration):\n",
      "[[1.78653462 0.3907992  2.48556875 1.11118626 0.57909784]\n",
      " [1.50973314 2.94279059 1.06883123 1.61401307 0.85916293]\n",
      " [0.97223838 1.52659791 1.71350289 1.12508597 0.73014447]\n",
      " [0.56464127 0.79924977 0.86265587 0.67954434 0.50002814]\n",
      " [0.3030649  0.41136783 0.43993885 0.37346986 0.29162985]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(V, policy, theta=1e-6):\n",
    "    stable_policy = False\n",
    "    while not stable_policy:\n",
    "        V = iterative_policy_evaluation(V)\n",
    "        stable_policy = True\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                old_action = policy[i, j]\n",
    "                action_values = [\n",
    "                    rewards[transitions[i, j, action][0], transitions[i, j, action][1]] + gamma * V[transitions[i, j, action][0], transitions[i, j, action][1]]\n",
    "                    for action in range(4)\n",
    "                ]\n",
    "                best_action = np.argmax(action_values)\n",
    "                policy[i, j] = best_action\n",
    "                if old_action != best_action:\n",
    "                    stable_policy = False\n",
    "    return policy, V\n",
    "\n",
    "policy = np.random.choice(4, size=(size, size))\n",
    "V_policy_iteration = np.zeros((size, size))\n",
    "policy, V_policy_iteration = policy_iteration(V_policy_iteration, policy)\n",
    "print(\"Optimal Policy (Policy Iteration):\")\n",
    "print(policy)\n",
    "print(\"Value Function (Policy Iteration):\")\n",
    "print(V_policy_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Value Iteration):\n",
      "[[3 0 2 2 2]\n",
      " [3 0 0 0 0]\n",
      " [3 0 2 0 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(V):\n",
    "    policy = np.zeros((size, size), dtype=int)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            action_values = [\n",
    "                rewards[transitions[i, j, action][0], transitions[i, j, action][1]] + gamma * V[transitions[i, j, action][0], transitions[i, j, action][1]]\n",
    "                for action in range(4)\n",
    "            ]\n",
    "            policy[i, j] = np.argmax(action_values)\n",
    "    return policy\n",
    "\n",
    "policy_from_value_iteration = policy_improvement(V_value_iteration)\n",
    "print(\"Optimal Policy (Value Iteration):\")\n",
    "print(policy_from_value_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the grid size\n",
    "size = 5\n",
    "\n",
    "# Initialize rewards with new conditions\n",
    "rewards = np.full((size, size), -0.2)\n",
    "rewards[0, 1] = 5  # Blue square\n",
    "rewards[0, 4] = 2.5  # Green square\n",
    "\n",
    "# Define terminal states (black squares)\n",
    "terminal_states = [(2, 4), (0, 4)]\n",
    "\n",
    "# Set rewards for terminal states\n",
    "for (i, j) in terminal_states:\n",
    "    rewards[i, j] = 0\n",
    "\n",
    "# Initialize transitions with terminal states\n",
    "def init_transitions_with_terminals():\n",
    "    transitions = np.zeros((size, size, 4, 2), dtype=int)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i, j) not in terminal_states:\n",
    "                transitions[i, j, 0] = [max(i-1, 0), j]  # Up\n",
    "                transitions[i, j, 1] = [min(i+1, size-1), j]  # Down\n",
    "                transitions[i, j, 2] = [i, max(j-1, 0)]  # Left\n",
    "                transitions[i, j, 3] = [i, min(j+1, size-1)]  # Right\n",
    "    # Special transitions\n",
    "    transitions[0, 1] = [[4, 1], [4, 1], [4, 1], [4, 1]]  # Blue to Red\n",
    "    transitions[1, 2] = [[2, 3], [2, 3], [2, 3], [2, 3]]  # Green to Yellow/Red with 0.5 prob each\n",
    "    return transitions\n",
    "\n",
    "transitions = init_transitions_with_terminals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Monte Carlo with Exploring Starts):\n",
      "[[3 0 2 2 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 3 3 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 3 0 0]]\n",
      "Value Function (Monte Carlo with Exploring Starts):\n",
      "[[ 0.81363842 -2.53918239  0.408184   -0.32191055  0.        ]\n",
      " [-0.74738658 -0.26458762 -1.27193994 -0.94761911 -0.47766531]\n",
      " [-1.78651188 -1.53589315 -1.7031521  -1.05957375  0.        ]\n",
      " [-2.3874413  -2.21964436 -2.11975938 -1.73386187 -1.24289107]\n",
      " [-2.61843231 -2.45097416 -2.28158504 -2.05246246 -1.81610831]]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_with_exploring_starts(episodes=500, gamma=0.95, epsilon=0.1):\n",
    "    V = np.zeros((size, size))\n",
    "    returns = {state: [] for state in [(i, j) for i in range(size) for j in range(size)]}\n",
    "    policy = np.random.choice(4, size=(size, size))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        # Generate an episode with exploring starts\n",
    "        start_state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        while start_state in terminal_states:\n",
    "            start_state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        state = start_state\n",
    "        episode = []\n",
    "        \n",
    "        while state not in terminal_states:\n",
    "            action = np.random.choice(4)\n",
    "            next_state = transitions[state[0], state[1], action]\n",
    "            reward = rewards[next_state[0], next_state[1]]\n",
    "            episode.append((state, action, reward))\n",
    "            state = tuple(next_state)\n",
    "        \n",
    "        G = 0\n",
    "        for t in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if not (state, action) in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "                action_values = [\n",
    "                    rewards[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]] + gamma * V[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]]\n",
    "                    for a in range(4)\n",
    "                ]\n",
    "                policy[state[0], state[1]] = np.argmax(action_values)\n",
    "                \n",
    "    return policy, V\n",
    "\n",
    "policy_mc_exploring, V_mc_exploring = monte_carlo_with_exploring_starts()\n",
    "print(\"Optimal Policy (Monte Carlo with Exploring Starts):\")\n",
    "print(policy_mc_exploring)\n",
    "print(\"Value Function (Monte Carlo with Exploring Starts):\")\n",
    "print(V_mc_exploring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Monte Carlo ε-soft):\n",
      "[[3 0 2 1 2]\n",
      " [0 0 0 0 2]\n",
      " [0 0 3 0 1]\n",
      " [0 0 0 2 2]\n",
      " [0 0 0 0 2]]\n",
      "Value Function (Monte Carlo ε-soft):\n",
      "[[ 4.55453277  0.87880265  4.15035268  2.93442971  0.        ]\n",
      " [ 3.97064728  1.50082547 -0.55465907  2.71091054  2.0463414 ]\n",
      " [ 2.21388036  1.78529323  1.40163956  2.05937958  0.        ]\n",
      " [ 1.11424808  1.0185867   0.62041245  1.64811808  0.03401989]\n",
      " [ 0.8227976   0.40633061 -0.3318594  -0.33342701 -0.51245531]]\n"
     ]
    }
   ],
   "source": [
    "def monte_carlo_e_soft(episodes=500, gamma=0.95, epsilon=0.1, alpha=0.1):\n",
    "    V = np.zeros((size, size))\n",
    "    returns = {state: [] for state in [(i, j) for i in range(size) for j in range(size)]}\n",
    "    policy = np.random.choice(4, size=(size, size))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        while state in terminal_states:\n",
    "            state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        episode = []\n",
    "        visited_state_actions = set()\n",
    "        \n",
    "        while state not in terminal_states:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(4)\n",
    "            else:\n",
    "                action = policy[state[0], state[1]]\n",
    "            next_state = transitions[state[0], state[1], action]\n",
    "            reward = rewards[next_state[0], next_state[1]]\n",
    "            episode.append((state, action, reward))\n",
    "            state = tuple(next_state)\n",
    "        \n",
    "        G = 0\n",
    "        for t in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in visited_state_actions:\n",
    "                visited_state_actions.add((state, action))\n",
    "                returns[state].append(G)\n",
    "                V[state] = V[state] + alpha * (G - V[state])\n",
    "                action_values = [\n",
    "                    rewards[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]] + gamma * V[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]]\n",
    "                    for a in range(4)\n",
    "                ]\n",
    "                policy[state[0], state[1]] = np.argmax(action_values)\n",
    "                \n",
    "    return policy, V\n",
    "\n",
    "policy_mc_e_soft, V_mc_e_soft = monte_carlo_e_soft()\n",
    "print(\"Optimal Policy (Monte Carlo ε-soft):\")\n",
    "print(policy_mc_e_soft)\n",
    "print(\"Value Function (Monte Carlo ε-soft):\")\n",
    "print(V_mc_e_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m policy, Q\n\u001b[0;32m---> 57\u001b[0m optimal_policy, Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_epsilon_soft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Policy (0=Up, 1=Down, 2=Left, 3=Right):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimal_policy)\n",
      "Cell \u001b[0;32mIn[22], line 52\u001b[0m, in \u001b[0;36mmonte_carlo_epsilon_soft\u001b[0;34m(num_episodes, epsilon, gamma, alpha)\u001b[0m\n\u001b[1;32m     50\u001b[0m         G \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m G \u001b[38;5;241m+\u001b[39m reward\n\u001b[1;32m     51\u001b[0m         returns_count[i, j, action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 52\u001b[0m         \u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (G \u001b[38;5;241m-\u001b[39m Q[i, j, action]) \u001b[38;5;241m/\u001b[39m returns_count[i, j, action]\n\u001b[1;32m     54\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy, Q\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Gridworld setup\n",
    "grid_size = 5\n",
    "gamma = 0.95  # Discount factor\n",
    "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Up, Down, Right, Left\n",
    "epsilon = 0.1  # Exploration factor\n",
    "alpha = 0.1  # Learning rate\n",
    "\n",
    "# Initialize rewards for all states with -0.2 default (white-to-white transitions)\n",
    "rewards_2 = np.full((grid_size, grid_size), -0.2)\n",
    "rewards_2[0, 1] = 5   # Blue square reward\n",
    "rewards_2[0, 4] = 2.5 # Green square reward\n",
    "\n",
    "# Terminal states setup\n",
    "is_terminal = np.zeros((grid_size, grid_size), dtype=bool)\n",
    "is_terminal[2, 4] = True  # Terminal state at location [2,4]\n",
    "is_terminal[4, 0] = True  # Terminal state at location [4,0]\n",
    "\n",
    "def get_reward(i, j, next_i, next_j):\n",
    "    \"\"\"Calculate and return the next state positions and reward.\"\"\"\n",
    "    if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "        return i, j, -0.5  # Penalty for stepping off the grid\n",
    "    if is_terminal[next_i, next_j]:\n",
    "        return next_i, next_j, 0  # No further rewards when moving into a terminal state\n",
    "    return next_i, next_j, rewards_2[next_i, next_j]  # Return the actual reward from the rewards matrix\n",
    "\n",
    "def monte_carlo_epsilon_soft(num_episodes=500, epsilon=0.1, gamma=0.95, alpha=0.1):\n",
    "    Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "    returns_count = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        i, j = random.randrange(grid_size), random.randrange(grid_size)\n",
    "        while is_terminal[i, j]:\n",
    "            i, j = random.randrange(grid_size), random.randrange(grid_size)\n",
    "        action = random.randrange(len(actions))\n",
    "        episode_data = []\n",
    "\n",
    "        while not is_terminal[i, j]:\n",
    "            next_i, next_j = i + actions[action][0], j + actions[action][1]\n",
    "            next_i, next_j, reward = get_reward(i, j, next_i, next_j)\n",
    "            episode_data.append((i, j, action, reward))\n",
    "            i, j = next_i, next_j\n",
    "            if not is_terminal[i, j]:\n",
    "                action = random.randrange(len(actions)) if random.random() < epsilon else np.argmax(Q[i, j])\n",
    "\n",
    "        G = 0\n",
    "        for i, j, action, reward in reversed(episode_data):\n",
    "            G = gamma * G + reward\n",
    "            returns_count[i, j, action] += 1\n",
    "            Q[i, j, action] += (G - Q[i, j, action]) / returns_count[i, j, action]\n",
    "\n",
    "    policy = np.argmax(Q, axis=2)\n",
    "    return policy, Q\n",
    "\n",
    "optimal_policy, Q_values = monte_carlo_epsilon_soft()\n",
    "\n",
    "print(\"Optimal Policy (0=Up, 1=Down, 2=Left, 3=Right):\")\n",
    "print(optimal_policy)\n",
    "print(\"\\nAssociated Value Function:\")\n",
    "print(np.max(Q_values, axis=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (0=Up, 1=Down, 2=Left, 3=Right):\n",
      "[[0 1 1 1 1]\n",
      " [3 3 3 1 1]\n",
      " [3 3 3 3 0]\n",
      " [2 3 1 2 1]\n",
      " [0 1 3 1 3]]\n",
      "\n",
      "Associated Value Function:\n",
      "[[45.93805436 43.16767849 45.47753596 42.66545446 34.5278757 ]\n",
      " [43.18151144 45.6284717  42.67901964 40.05003104 36.25873461]\n",
      " [40.59214757 42.75957338 39.96061083 37.98457759  0.        ]\n",
      " [ 0.         39.60045647 34.47569908 24.06525325  4.15283234]\n",
      " [ 0.          0.         31.69450345 26.57017019  2.99042306]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Gridworld setup\n",
    "grid_size = 5\n",
    "gamma = 0.95  # Discount factor\n",
    "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Up, Down, Right, Left\n",
    "epsilon = 0.1  # Exploration factor\n",
    "\n",
    "# Initialize rewards for all states with -0.2 default (white-to-white transitions)\n",
    "rewards = np.full((grid_size, grid_size), -0.2)\n",
    "rewards[0, 1] = 5   # Blue square reward\n",
    "rewards[0, 4] = 2.5 # Green square reward\n",
    "\n",
    "# Terminal states setup\n",
    "is_terminal = np.zeros((grid_size, grid_size), dtype=bool)\n",
    "is_terminal[2, 4] = True  # Terminal state at location [2,4]\n",
    "is_terminal[4, 0] = True  # Terminal state at location [4,0]\n",
    "\n",
    "def get_reward(i, j, next_i, next_j):\n",
    "    \"\"\"Calculate and return the next state positions and reward.\"\"\"\n",
    "    if next_i < 0 or next_i >= grid_size or next_j < 0 or next_j >= grid_size:\n",
    "        return i, j, -0.5  # Penalty for stepping off the grid\n",
    "    if is_terminal[next_i, next_j]:\n",
    "        return next_i, next_j, 0  # No further rewards when moving into a terminal state\n",
    "    return next_i, next_j, rewards[next_i, next_j]  # Return the actual reward from the rewards matrix\n",
    "\n",
    "def monte_carlo_epsilon_soft(num_episodes=500, epsilon=0.1, gamma=0.95):\n",
    "    Q = np.zeros((grid_size, grid_size, len(actions)))\n",
    "    returns_count = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Ensure starting state is not terminal\n",
    "        while True:\n",
    "            i, j = random.randrange(grid_size), random.randrange(grid_size)\n",
    "            if not is_terminal[i, j]:\n",
    "                break\n",
    "        \n",
    "        episode_data = []\n",
    "        while not is_terminal[i, j]:\n",
    "            action = random.randrange(len(actions)) if random.random() < epsilon else np.argmax(Q[i, j])\n",
    "            next_i, next_j = i + actions[action][0], j + actions[action][1]\n",
    "            next_i, next_j, reward = get_reward(i, j, next_i, next_j)\n",
    "            episode_data.append((i, j, action, reward))\n",
    "            i, j = next_i, next_j\n",
    "        \n",
    "        G = 0\n",
    "        for i, j, action, reward in reversed(episode_data):\n",
    "            G = gamma * G + reward\n",
    "            returns_count[i, j, action] += 1\n",
    "            Q[i, j, action] += (G - Q[i, j, action]) / returns_count[i, j, action]\n",
    "\n",
    "    policy = np.argmax(Q, axis=2)\n",
    "    return policy, Q\n",
    "\n",
    "# Reduced number of episodes for faster runtime (for testing)\n",
    "optimal_policy, Q_values = monte_carlo_epsilon_soft(num_episodes=100)\n",
    "\n",
    "print(\"Optimal Policy (0=Up, 1=Down, 2=Left, 3=Right):\")\n",
    "print(optimal_policy)\n",
    "print(\"\\nAssociated Value Function:\")\n",
    "print(np.max(Q_values, axis=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Importance Sampling):\n",
      "[[[0.25  0.25  0.25  0.25 ]\n",
      "  [0.25  0.25  0.25  0.25 ]\n",
      "  [0.25  0.25  0.25  0.25 ]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.25  0.25  0.25  0.25 ]]\n",
      "\n",
      " [[0.25  0.25  0.25  0.25 ]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.025 0.025 0.925 0.025]\n",
      "  [0.25  0.25  0.25  0.25 ]]\n",
      "\n",
      " [[0.925 0.025 0.025 0.025]\n",
      "  [0.025 0.025 0.025 0.925]\n",
      "  [0.25  0.25  0.25  0.25 ]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.925 0.025 0.025 0.025]]\n",
      "\n",
      " [[0.025 0.925 0.025 0.025]\n",
      "  [0.025 0.025 0.025 0.925]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.25  0.25  0.25  0.25 ]\n",
      "  [0.025 0.025 0.925 0.025]]\n",
      "\n",
      " [[0.25  0.25  0.25  0.25 ]\n",
      "  [0.025 0.025 0.925 0.025]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.925 0.025 0.025 0.025]\n",
      "  [0.925 0.025 0.025 0.025]]]\n",
      "Value Function (Importance Sampling):\n",
      "[[ 0.          0.          0.          1.9945      0.        ]\n",
      " [ 0.         -0.2        -0.2         1.1038961   0.        ]\n",
      " [-0.2        -0.02069416  0.          0.         -0.2       ]\n",
      " [-0.39       -0.2        -0.01946335  0.         -0.01380736]\n",
      " [ 0.         -0.39       -0.20490449 -0.0352112  -0.21076487]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize grid size and discount factor\n",
    "size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Initialize rewards and transitions with terminal states\n",
    "def init_environment():\n",
    "    rewards = np.full((size, size), -0.2)\n",
    "    rewards[0, 1] = 5\n",
    "    rewards[1, 2] = 2.5\n",
    "    terminal_states = [(2, 2), (3, 3)]\n",
    "    for (i, j) in terminal_states:\n",
    "        rewards[i, j] = 0\n",
    "\n",
    "    transitions = np.zeros((size, size, 4, 2), dtype=int)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i, j) not in terminal_states:\n",
    "                transitions[i, j, 0] = [max(i-1, 0), j]  # Up\n",
    "                transitions[i, j, 1] = [min(i+1, size-1), j]  # Down\n",
    "                transitions[i, j, 2] = [i, max(j-1, 0)]  # Left\n",
    "                transitions[i, j, 3] = [i, min(j+1, size-1)]  # Right\n",
    "\n",
    "    transitions[0, 1] = [[4, 1], [4, 1], [4, 1], [4, 1]]\n",
    "    transitions[1, 2] = [[2, 3], [2, 3], [2, 3], [2, 3]]\n",
    "\n",
    "    return rewards, transitions, terminal_states\n",
    "\n",
    "rewards, transitions, terminal_states = init_environment()\n",
    "\n",
    "# Policy iteration with importance sampling\n",
    "def policy_iteration_importance_sampling(episodes=1000, gamma=0.95, epsilon=0.1):\n",
    "    V = np.zeros((size, size))\n",
    "    # Initialize policy with equal probability for all actions\n",
    "    policy = np.full((size, size, 4), 0.25)\n",
    "    C = np.zeros((size, size))\n",
    "\n",
    "    def generate_episode(policy):\n",
    "        state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        while state in terminal_states:\n",
    "            state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "        episode = []\n",
    "        \n",
    "        while state not in terminal_states:\n",
    "            action = np.random.choice(4, p=policy[state[0], state[1]])\n",
    "            next_state = transitions[state[0], state[1], action]\n",
    "            reward = rewards[next_state[0], next_state[1]]\n",
    "            episode.append((state, action, reward))\n",
    "            state = tuple(next_state)\n",
    "        \n",
    "        return episode\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        for t in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            C[state[0], state[1]] += W\n",
    "            V[state[0], state[1]] += (W / C[state[0], state[1]]) * (G - V[state[0], state[1]])\n",
    "            \n",
    "            optimal_action = np.argmax([\n",
    "                rewards[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]] + gamma * V[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]]\n",
    "                for a in range(4)\n",
    "            ])\n",
    "            policy[state[0], state[1]] = np.full(4, epsilon / 4)\n",
    "            policy[state[0], state[1]][optimal_action] += 1 - epsilon\n",
    "            \n",
    "            if action != optimal_action:\n",
    "                break\n",
    "            W *= 1.0 / policy[state[0], state[1]][action]\n",
    "            \n",
    "    return policy, V\n",
    "\n",
    "policy_importance_sampling, V_importance_sampling = policy_iteration_importance_sampling()\n",
    "print(\"Optimal Policy (Importance Sampling):\")\n",
    "print(policy_importance_sampling)\n",
    "print(\"Value Function (Importance Sampling):\")\n",
    "print(V_importance_sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Optimal Policy:\n",
      "[[[0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [1. 0. 0. 0.]]]\n",
      "Q-values:\n",
      "[[[0.         0.         6.69253731 7.375     ]\n",
      "  [2.5        2.5        3.45       4.50555556]\n",
      "  [2.375      0.         8.4806383  0.        ]\n",
      "  [5.25901235 8.19231265 5.23278689 4.03552472]\n",
      "  [0.         4.17826616 0.68113208 0.        ]]\n",
      "\n",
      " [[0.         0.         0.         1.58333333]\n",
      "  [8.8044186  8.5486484  0.         0.        ]\n",
      "  [8.22179692 3.5815     5.76263736 1.35714286]\n",
      "  [7.78689533 8.46351347 7.93913113 3.89675177]\n",
      "  [0.         4.01638889 4.24945518 1.03142857]]\n",
      "\n",
      " [[2.40666667 3.83891318 3.4295     0.        ]\n",
      "  [8.6761487  0.         0.         1.58333333]\n",
      "  [2.74177215 0.85952381 0.         3.70333333]\n",
      "  [3.28358974 6.07037698 3.23345455 0.71028037]\n",
      "  [4.17033776 1.6559633  3.15636364 4.05023888]]\n",
      "\n",
      " [[3.6785306  0.         0.         0.        ]\n",
      "  [1.805      0.         3.47915778 1.06176471]\n",
      "  [3.12821429 0.         2.572125   3.88157893]\n",
      "  [6.10333538 2.84396265 3.95832163 1.71904762]\n",
      "  [1.0754717  0.         1.5        0.        ]]\n",
      "\n",
      " [[0.         0.         0.         0.        ]\n",
      "  [3.31634311 0.         0.         0.        ]\n",
      "  [0.         0.         3.15082637 1.8278481 ]\n",
      "  [1.99193548 0.         2.99341152 0.        ]\n",
      "  [0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Reward structure\n",
    "rewards = np.zeros((grid_size, grid_size))\n",
    "rewards[0, 1] = 5  # Blue square\n",
    "rewards[2, 3] = 2.5  # Green square\n",
    "# More reward definitions as needed\n",
    "\n",
    "# Transition probabilities\n",
    "# We use a dictionary to hold transitions for special states\n",
    "transitions = {\n",
    "    (0, 1): (2, 3),  # Blue to Red\n",
    "    # More transitions as needed\n",
    "}\n",
    "\n",
    "# Initialize value function and policy\n",
    "Q = np.zeros((grid_size, grid_size, 4))\n",
    "policy = np.ones((grid_size, grid_size, 4)) * 0.25  # Equiprobable random policy\n",
    "\n",
    "# Actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "\n",
    "# Function to get next state\n",
    "def get_next_state(state, action):\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return state\n",
    "    return next_state\n",
    "\n",
    "# Generate an episode using the behavior policy\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    while len(episode) < 20:  # limit the length of the episode\n",
    "        action_idx = np.random.choice(4, p=policy[state])\n",
    "        action = actions[action_idx]\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        if state in transitions:\n",
    "            next_state = transitions[state]\n",
    "            reward = rewards[next_state]\n",
    "        episode.append((state, action_idx, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Off-Policy Monte Carlo Control with Weighted Importance Sampling\n",
    "def off_policy_mc_control(policy, Q, gamma, episodes=1000):\n",
    "    C = np.zeros_like(Q)  # Cumulative sum of importance weights\n",
    "    \n",
    "    # Target policy initialization (deterministic)\n",
    "    target_policy = np.zeros_like(policy)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            target_policy[i, j, np.argmax(Q[i, j])] = 1.0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        for (state, action_idx, reward) in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            C[state][action_idx] += W\n",
    "            Q[state][action_idx] += (W / C[state][action_idx]) * (G - Q[state][action_idx])\n",
    "            target_policy[state] = np.eye(4)[np.argmax(Q[state])]\n",
    "            if action_idx != np.argmax(target_policy[state]):\n",
    "                break\n",
    "            W /= policy[state][action_idx]\n",
    "    \n",
    "    # Update the policy to be greedy with respect to Q\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            best_action = np.argmax(Q[i, j])\n",
    "            policy[i, j] = np.eye(4)[best_action]\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "# Initialize policy and Q-values\n",
    "policy = np.ones((grid_size, grid_size, 4)) * 0.25  # Equiprobable random policy\n",
    "Q = np.zeros((grid_size, grid_size, 4))\n",
    "\n",
    "# Learn the optimal policy\n",
    "optimal_policy, Q_values = off_policy_mc_control(policy, Q, gamma)\n",
    "\n",
    "# Print the learned optimal policy\n",
    "print(\"Learned Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"Q-values:\")\n",
    "print(Q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Optimal Policy:\n",
      "[[[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [1. 0. 0. 0.]]]\n",
      "Q-values:\n",
      "[[[ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]]\n",
      "\n",
      " [[ 0.   0.   0.  -0.2]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.  -0.2  0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]]\n",
      "\n",
      " [[ 0.   0.   0.   0. ]\n",
      "  [-0.2  0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.  -0.2  0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]]\n",
      "\n",
      " [[ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.  -0.2]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.  -0.2  0. ]]\n",
      "\n",
      " [[ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]\n",
      "  [-0.2  0.   0.   0. ]\n",
      "  [ 0.   0.   0.   0. ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = 5\n",
    "gamma = 0.95\n",
    "\n",
    "# Reward structure\n",
    "rewards = np.full((grid_size, grid_size), -0.2)  # All white squares\n",
    "rewards[0, 1] = 5  # Blue square\n",
    "rewards[2, 3] = 2.5  # Green square\n",
    "# Adding terminal states\n",
    "terminal_states = [(1, 1), (3, 3)]  # Example terminal states\n",
    "\n",
    "# Transition probabilities\n",
    "# We use a dictionary to hold transitions for special states\n",
    "transitions = {\n",
    "    (0, 1): (2, 3),  # Blue to Red\n",
    "    # More transitions as needed\n",
    "}\n",
    "\n",
    "# Initialize value function and policy\n",
    "Q = np.zeros((grid_size, grid_size, 4))\n",
    "policy = np.ones((grid_size, grid_size, 4)) * 0.25  # Equiprobable random policy\n",
    "\n",
    "# Actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "\n",
    "# Function to get next state\n",
    "def get_next_state(state, action):\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "        return state\n",
    "    return next_state\n",
    "\n",
    "# Check if state is terminal\n",
    "def is_terminal(state):\n",
    "    return state in terminal_states\n",
    "\n",
    "# Generate an episode using the behavior policy\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    while not is_terminal(state):\n",
    "        action_idx = np.random.choice(4, p=policy[state])\n",
    "        action = actions[action_idx]\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        if state in transitions:\n",
    "            next_state = transitions[state]\n",
    "            reward = rewards[next_state]\n",
    "        episode.append((state, action_idx, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Off-Policy Monte Carlo Control with Weighted Importance Sampling\n",
    "def off_policy_mc_control(policy, Q, gamma, episodes=1000):\n",
    "    C = np.zeros_like(Q)\n",
    "    target_policy = np.zeros((grid_size, grid_size, 4))\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            target_policy[i, j, np.argmax(Q[i, j])] = 1.0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        for (state, action_idx, reward) in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            C[state][action_idx] += W\n",
    "            Q[state][action_idx] += (W / C[state][action_idx]) * (G - Q[state][action_idx])\n",
    "            target_policy[state] = np.eye(4)[np.argmax(Q[state])]\n",
    "            if action_idx != np.argmax(target_policy[state]):\n",
    "                break\n",
    "            W /= policy[state][action_idx]\n",
    "    \n",
    "    # Update the policy to be greedy with respect to Q\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            best_action = np.argmax(Q[i, j])\n",
    "            policy[i, j] = np.eye(4)[best_action]\n",
    "    \n",
    "    return policy, Q\n",
    "\n",
    "# Initialize policy and Q-values\n",
    "policy = np.ones((grid_size, grid_size, 4)) * 0.25  # Equiprobable random policy\n",
    "Q = np.zeros((grid_size, grid_size, 4))\n",
    "\n",
    "# Learn the optimal policy using off-policy Monte Carlo control\n",
    "optimal_policy, Q_values = off_policy_mc_control(policy, Q, gamma)\n",
    "\n",
    "# Print the learned optimal policy\n",
    "print(\"Learned Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"Q-values:\")\n",
    "print(Q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Permuted Squares):\n",
      "[[0 0 1 2 2]\n",
      " [3 3 0 2 2]\n",
      " [0 0 0 0 0]\n",
      " [1 1 0 0 2]\n",
      " [1 2 3 0 2]]\n",
      "Value Function (Permuted Squares):\n",
      "[[-0.74345619 -1.13325016 -0.12854713 -0.42643028 -0.58721484]\n",
      " [-0.85251428 -0.46594164 -0.98157062 -0.23762868 -0.76983498]\n",
      " [-1.01936152 -0.99452445 -0.90628338 -0.82270592 -0.99277685]\n",
      " [-1.05085445 -1.16430589 -1.03248784 -0.90628338 -0.983627  ]\n",
      " [-0.85396306 -0.98236859 -0.94939194 -0.84526219 -0.85650213]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration_permuted_squares(episodes=500, gamma=0.95):\n",
    "    V = np.zeros((size, size))\n",
    "    policy = np.random.choice(4, size=(size, size))\n",
    "\n",
    "    def permute_squares():\n",
    "        if np.random.rand() < 0.5:\n",
    "            return [(1, 2), (0, 1)], [(0, 1), (1, 2)]\n",
    "        else:\n",
    "            return [(0, 1), (1, 2)], [(1, 2), (0, 1)]\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        blue_squares, green_squares = permute_squares()\n",
    "        \n",
    "        for _ in range(100):  # Adjust number of iterations\n",
    "            policy, V = policy_iteration(V, policy)\n",
    "            rewards[blue_squares[0][0], blue_squares[0][1]] = 5\n",
    "            rewards[blue_squares[1][0], blue_squares[1][1]] = -0.2\n",
    "            rewards[green_squares[0][0], green_squares[0][1]] = 2.5\n",
    "            rewards[green_squares[1][0], green_squares[1][1]] = -0.2\n",
    "        \n",
    "    return policy, V\n",
    "\n",
    "policy_permuted_squares, V_permuted_squares = policy_iteration_permuted_squares()\n",
    "print(\"Optimal Policy (Permuted Squares):\")\n",
    "print(policy_permuted_squares)\n",
    "print(\"Value Function (Permuted Squares):\")\n",
    "print(V_permuted_squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def policy_iteration_importance_sampling(V, policy, episodes=500, gamma=0.95):\n",
    "#     def generate_episode(policy):\n",
    "#         state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "#         while state in terminal_states:\n",
    "#             state = (np.random.randint(0, size), np.random.randint(0, size))\n",
    "#         episode = []\n",
    "        \n",
    "#         while state not in terminal_states:\n",
    "#             action = policy[state[0], state[1]]\n",
    "#             next_state = transitions[state[0], state[1], action]\n",
    "#             reward = rewards[next_state[0], next_state[1]]\n",
    "#             episode.append((state, action, reward))\n",
    "#             state = tuple(next_state)\n",
    "        \n",
    "#         return episode\n",
    "\n",
    "#     def compute_importance_sampling_ratios(episode, policy, behavior_policy):\n",
    "#         ratios = []\n",
    "#         for state, action, reward in episode:\n",
    "#             if behavior_policy[state[0], state[1]] == action:\n",
    "#                 ratios.append(1.0)\n",
    "#             else:\n",
    "#                 ratios.append(0.0)\n",
    "#         return np.prod(ratios)\n",
    "\n",
    "#     for _ in range(episodes):\n",
    "#         behavior_policy = np.random.choice(4, size=(size, size))\n",
    "#         episode = generate_episode(behavior_policy)\n",
    "#         G = 0\n",
    "#         W = 1\n",
    "        \n",
    "#         for t in range(len(episode)-1, -1, -1):\n",
    "#             state, action, reward = episode[t]\n",
    "#             G = gamma * G + reward\n",
    "#             V[state[0], state[1]] += (W / np.sum(W)) * (G - V[state[0], state[1]])\n",
    "#             policy[state[0], state[1]] = np.argmax([\n",
    "#                 rewards[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]] + gamma * V[transitions[state[0], state[1], a][0], transitions[state[0], state[1], a][1]]\n",
    "#                 for a in range(4)\n",
    "#             ])\n",
    "#             W *= 1.0 / (0.25)  # Assume equiprobable behavior policy\n",
    "            \n",
    "#     return policy, V\n",
    "\n",
    "# policy_importance_sampling = np.random.choice(4, size=(size, size))\n",
    "# V_importance_sampling = np.zeros((size, size))\n",
    "# policy_importance_sampling, V_importance_sampling = policy_iteration_importance_sampling(V_importance_sampling, policy_importance_sampling)\n",
    "# print(\"Optimal Policy (Importance Sampling):\")\n",
    "# print(policy_importance_sampling)\n",
    "# print(\"Value Function (Importance Sampling):\")\n",
    "# print(V_importance_sampling)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
